#!/usr/bin/env python3
"""
Experiment Analysis: Pine Hollow Mystery vs Anime Baseline
Compare GPT-2 performance across different story themes
"""

import pandas as pd
import numpy as np
import os

def analyze_experiments():
    print("ğŸ“Š CYOA EXPERIMENT ANALYSIS")
    print("=" * 60)
    
    # Load both experiment results
    if os.path.exists('generated_stories_pine_hollow_baseline.csv'):
        mystery_df = pd.read_csv('generated_stories_pine_hollow_baseline.csv')
        print(f"ğŸŒ² Pine Hollow Mystery: {len(mystery_df)} scenarios")
    else:
        print("âŒ Pine Hollow results not found")
        return
    
    if os.path.exists('generated_stories_baseline.csv'):
        anime_df = pd.read_csv('generated_stories_baseline.csv')
        print(f"ğŸŒ Anime Baseline: {len(anime_df)} scenarios")
    else:
        print("âš ï¸ Anime baseline not found for comparison")
        anime_df = None
    
    print("\n" + "=" * 60)
    print("ğŸ” PINE HOLLOW MYSTERY ANALYSIS")
    print("=" * 60)
    
    # Mystery experiment analysis
    print(f"\nğŸ“ˆ Basic Metrics:")
    print(f"   â€¢ Stories generated: {len(mystery_df)}")
    print(f"   â€¢ Avg response length: {mystery_df['ai_response'].str.len().mean():.1f} chars")
    print(f"   â€¢ Twin Peaks scenarios: {len(mystery_df[mystery_df['atmosphere_level'] == 'twin_peaks'])}")
    print(f"   â€¢ Stranger Things scenarios: {len(mystery_df[mystery_df['atmosphere_level'] == 'stranger_things'])}")
    
    print(f"\nğŸ­ Dialogue Style Distribution:")
    for style in mystery_df['dialogue_style'].unique():
        count = len(mystery_df[mystery_df['dialogue_style'] == style])
        print(f"   â€¢ {style}: {count} scenarios")
    
    print(f"\nğŸŒ² Story Branch Distribution:")
    for branch in mystery_df['branch_type'].unique():
        count = len(mystery_df[mystery_df['branch_type'] == branch])
        print(f"   â€¢ {branch}: {count} scenarios")
    
    # Quality analysis
    print(f"\nğŸ§  QUALITY OBSERVATIONS:")
    
    # Check for coherence issues
    short_responses = mystery_df[mystery_df['ai_response'].str.len() < 100]
    long_responses = mystery_df[mystery_df['ai_response'].str.len() > 500]
    
    print(f"   â€¢ Short responses (<100 chars): {len(short_responses)}")
    print(f"   â€¢ Long responses (>500 chars): {len(long_responses)}")
    
    # Sample analysis
    print(f"\nğŸ“ SAMPLE RESPONSE ANALYSIS:")
    
    for idx in range(min(3, len(mystery_df))):
        row = mystery_df.iloc[idx]
        ai_response = row['ai_response']
        
        print(f"\nğŸ¬ Sample {idx + 1} ({row['atmosphere_level']} - {row['dialogue_style']}):")
        print(f"   ğŸ“– Prompt: {row['prompt'][:80]}...")
        print(f"   ğŸ‘¤ Human: {row['human_response'][:80]}...")
        print(f"   ğŸ¤– AI: {ai_response[:80]}...")
        
        # Quality indicators
        coherent = "Detective" in ai_response or "sheriff" in ai_response.lower() or "coffee" in ai_response.lower()
        on_topic = any(word in ai_response.lower() for word in ['sarah', 'pine', 'mystery', 'disappeared', 'town'])
        atmosphere = any(word in ai_response.lower() for word in ['forest', 'fog', 'dark', 'strange', 'whisper'])
        
        print(f"   âœ… Mystery elements: {'Yes' if on_topic else 'No'}")
        print(f"   ğŸ­ Character consistency: {'Yes' if coherent else 'No'}")
        print(f"   ğŸŒ«ï¸ Atmospheric: {'Yes' if atmosphere else 'No'}")
    
    # Comparison with anime if available
    if anime_df is not None:
        print("\n" + "=" * 60)
        print("ğŸ†š MYSTERY vs ANIME COMPARISON")
        print("=" * 60)
        
        print(f"\nğŸ“Š Length Comparison:")
        mystery_avg = mystery_df['ai_response'].str.len().mean()
        anime_avg = anime_df['ai_response'].str.len().mean()
        print(f"   â€¢ Mystery avg: {mystery_avg:.1f} chars")
        print(f"   â€¢ Anime avg: {anime_avg:.1f} chars")
        print(f"   â€¢ Difference: {mystery_avg - anime_avg:+.1f} chars")
        
        print(f"\nğŸ¯ Theme Consistency:")
        # Check for theme-appropriate keywords
        mystery_keywords = ['detective', 'sheriff', 'mystery', 'disappeared', 'town', 'coffee']
        anime_keywords = ['magic', 'academy', 'guild', 'festival', 'cherry', 'spirits']
        
        mystery_theme_score = mystery_df['ai_response'].apply(
            lambda x: sum(1 for kw in mystery_keywords if kw in x.lower())
        ).mean()
        
        anime_theme_score = anime_df['ai_response'].apply(
            lambda x: sum(1 for kw in anime_keywords if kw in x.lower())
        ).mean()
        
        print(f"   â€¢ Mystery theme words per response: {mystery_theme_score:.2f}")
        print(f"   â€¢ Anime theme words per response: {anime_theme_score:.2f}")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ KEY FINDINGS & NEXT STEPS")
    print("=" * 60)
    
    print(f"\nğŸ” Observations:")
    print(f"   â€¢ GPT-2 generates long, somewhat coherent responses")
    print(f"   â€¢ Mystery atmosphere partially maintained")
    print(f"   â€¢ Character consistency needs improvement")
    print(f"   â€¢ Twin Peaks dialogue patterns not well captured")
    
    print(f"\nğŸ“ˆ Recommendations:")
    print(f"   1. Fine-tune on mystery/thriller literature")
    print(f"   2. Add more Twin Peaks dialogue examples")
    print(f"   3. Implement better prompt engineering")
    print(f"   4. Create evaluation metrics for atmosphere")
    print(f"   5. Expand dataset to 20+ scenarios per branch")
    
    print(f"\nğŸš€ Portfolio Value:")
    print(f"   âœ… Shows domain adaptation (anime â†’ mystery)")
    print(f"   âœ… Demonstrates systematic experimentation")
    print(f"   âœ… Unique theme (Twin Peaks + ML is rare!)")
    print(f"   âœ… Clear progression path for improvement")
    
    print(f"\nğŸŒ Next: Check MLflow UI at http://localhost:5000")

if __name__ == "__main__":
    analyze_experiments() 