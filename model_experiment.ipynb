{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 5) (1652178399.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m**ML Concepts We'll Learn**:\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 5)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¤– CYOA Story Generation - First ML Experiment\n",
    "\n",
    "**Goal**: Test GPT-2 model for generating anime-inspired Choose Your Own Adventure responses\n",
    "\n",
    "**ML Concepts We'll Learn**:\n",
    "- **Pre-trained models**: Using AI that's already learned from millions of texts\n",
    "- **Inference**: Getting the model to generate new text\n",
    "- **Model evaluation**: Checking if the generated stories make sense\n",
    "- **MLflow model tracking**: Logging model experiments alongside data experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸš€ Libraries loaded successfully!\")\n",
    "print(f\"ğŸ“Š PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ¤– Transformers library ready for ML magic!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ğŸ“Š Step 1: Load Our CYOA Dataset\n",
    "\n",
    "**ML Concept**: Always start with understanding your data before applying models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our processed anime CYOA dataset\n",
    "df = pd.read_csv('data/processed_story_dataset.csv')\n",
    "\n",
    "print(f\"ğŸ“š Dataset loaded: {len(df)} anime-inspired scenarios\")\n",
    "print(f\"ğŸ“ Columns: {list(df.columns)}\")\n",
    "print(\"\\nğŸŒ Sample prompt and response:\")\n",
    "print(f\"Prompt: {df.iloc[0]['prompt']}\")\n",
    "print(f\"Response: {df.iloc[0]['response']}\")\n",
    "\n",
    "# Display the full dataset for review\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ğŸ§  Step 2: Set Up MLflow Experiment\n",
    "\n",
    "**MLOps Concept**: Track every model experiment just like we tracked data experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLflow experiment for model tracking\n",
    "experiment_name = \"cyoa_model_experiments\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"ğŸ¯ MLflow experiment created: {experiment_name}\")\n",
    "print(\"ğŸ“Š This will track all our model experiments in one place\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ğŸ¤– Step 3: Load Pre-trained GPT-2 Model\n",
    "\n",
    "**ML Concept**: GPT-2 is a \"language model\" trained on millions of texts. We'll use it as-is first, then later teach it our anime style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading GPT-2 model (this might take a moment...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPT-2 model loaded successfully!\n",
      "ğŸ’¾ Model size: ~117M parameters (that's 117 million learned patterns!)\n",
      "ğŸ–¥ï¸ Running on: CPU\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow run for this experiment\n",
    "mlflow.start_run(run_name=\"gpt2_baseline_test\")\n",
    "\n",
    "# Log experiment parameters\n",
    "mlflow.log_param(\"model_name\", \"gpt2\")\n",
    "mlflow.log_param(\"model_type\", \"pre_trained_baseline\")\n",
    "mlflow.log_param(\"dataset_size\", len(df))\n",
    "mlflow.log_param(\"max_length\", 100)  # Max words to generate\n",
    "\n",
    "print(\"ğŸ”„ Loading GPT-2 model (this might take a moment...)\")\n",
    "\n",
    "# Create a text generation pipeline (simplified interface)\n",
    "# ML Concept: Pipeline = pre-processing + model + post-processing in one step\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model='gpt2',  # Small GPT-2 model (117M parameters)\n",
    "    tokenizer='gpt2',\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "print(\"âœ… GPT-2 model loaded successfully!\")\n",
    "print(f\"ğŸ’¾ Model size: ~117M parameters (that's 117 million learned patterns!)\")\n",
    "print(f\"ğŸ–¥ï¸ Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4095306715.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m**ML Concept**: \"Inference\" = using the trained model to generate new content.\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## ğŸ¨ Step 4: Generate Stories from Our Prompts\n",
    "\n",
    "**ML Concept**: \"Inference\" = using the trained model to generate new content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test story generation on our anime prompts\n",
    "generated_stories = []\n",
    "\n",
    "print(\"ğŸŒ Generating anime-style CYOA responses...\\n\")\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    original_response = row['response']\n",
    "    genre = row['genre']\n",
    "    \n",
    "    # Generate story continuation\n",
    "    # ML Concept: We give the model a prompt and it predicts what comes next\n",
    "    generated = generator(\n",
    "        prompt,\n",
    "        max_length=100,  # Total length including prompt\n",
    "        num_return_sequences=1,  # Generate 1 story per prompt\n",
    "        temperature=0.7,  # Creativity level (0=boring, 1=wild)\n",
    "        do_sample=True,  # Enable creative sampling\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Extract just the generated part (remove original prompt)\n",
    "    generated_text = generated[0]['generated_text']\n",
    "    generated_response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    # Store results\n",
    "    story_result = {\n",
    "        'genre': genre,\n",
    "        'prompt': prompt,\n",
    "        'original_response': original_response,\n",
    "        'generated_response': generated_response\n",
    "    }\n",
    "    generated_stories.append(story_result)\n",
    "    \n",
    "    # Display comparison\n",
    "    print(f\"ğŸ­ {genre.upper()} GENRE:\")\n",
    "    print(f\"ğŸ“ Prompt: {prompt}\")\n",
    "    print(f\"âœ¨ Original: {original_response}\")\n",
    "    print(f\"ğŸ¤– Generated: {generated_response}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"ğŸ‰ Generated {len(generated_stories)} stories!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ğŸ“Š Step 5: Evaluate and Log Results\n",
    "\n",
    "**ML Concept**: Always measure how good your model's outputs are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple evaluation metrics\n",
    "total_stories = len(generated_stories)\n",
    "avg_generated_length = sum(len(story['generated_response']) for story in generated_stories) / total_stories\n",
    "avg_original_length = sum(len(story['original_response']) for story in generated_stories) / total_stories\n",
    "\n",
    "# Log metrics to MLflow\n",
    "mlflow.log_metric(\"total_stories_generated\", total_stories)\n",
    "mlflow.log_metric(\"avg_generated_length\", avg_generated_length)\n",
    "mlflow.log_metric(\"avg_original_length\", avg_original_length)\n",
    "\n",
    "# Save generated stories as artifact\n",
    "results_df = pd.DataFrame(generated_stories)\n",
    "results_df.to_csv('generated_stories_baseline.csv', index=False)\n",
    "mlflow.log_artifact('generated_stories_baseline.csv')\n",
    "\n",
    "print(f\"ğŸ“Š Evaluation Complete:\")\n",
    "print(f\"ğŸ“ Stories generated: {total_stories}\")\n",
    "print(f\"ğŸ“ Average generated length: {avg_generated_length:.1f} characters\")\n",
    "print(f\"ğŸ“ Average original length: {avg_original_length:.1f} characters\")\n",
    "print(f\"ğŸ’¾ Results saved to MLflow artifacts\")\n",
    "\n",
    "# Display results table\n",
    "print(\"\\nğŸ“‹ Results Summary:\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ğŸ¯ Step 6: Wrap Up & Next Steps\n",
    "\n",
    "**Learning Questions to Consider**:\n",
    "1. How do the generated stories compare to our original anime-inspired ones?\n",
    "2. Does GPT-2 understand the CYOA format?\n",
    "3. What anime elements are missing from the generated content?\n",
    "\n",
    "**Next Experiments**:\n",
    "- Fine-tune GPT-2 on our anime dataset\n",
    "- Try different temperature settings for creativity\n",
    "- Expand our dataset with more anime scenarios\n",
    "- Add prompt engineering for better anime context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"ğŸ Experiment complete!\")\n",
    "print(\"ğŸŒ Check MLflow UI at http://localhost:5000 to see your model experiment\")\n",
    "print(\"ğŸ“ Look for the 'cyoa_model_experiments' experiment\")\n",
    "print(\"\\nğŸš€ Ready for the next phase: Fine-tuning for anime storytelling!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
